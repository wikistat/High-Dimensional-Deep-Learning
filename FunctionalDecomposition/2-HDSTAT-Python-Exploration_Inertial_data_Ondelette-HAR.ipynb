{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance d'Activité Humaine](https://github.com/wikistat/Ateliers-Big-Data/5-HumanActivityRecognition) ([*HAR*](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)) en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>   Seconde partie:  étude des signaux bruts -  Exploration et prédiction avec les coefficients dans une base d'ondelettes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Introduction\n",
    "##  Contexte\n",
    "Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*) à partir d’enregistrements, par exemple du gyroscope et de l'accéléromètre d'un smartphone, objet connecté précurseur et dont la fonctionnalité de téléphonie devient très secondaire.\n",
    "Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  \n",
    "\n",
    "Les données publiques disponibles et largement étudiées ont été acquises, décrites et analysées par [Anguita et al. (2013)]().\n",
    "Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine ainsi que sur le site *Kaggle*.\n",
    "\n",
    "L'archive contient les données brutes: accélérations en x, y, et z, chacun de 128 colonnes. D'autres fichiers en y soustrayant la gravité naturelle ainsi que les accélérations angulaires en x, y, et z soit en tout 9 fichiers. Mais 6 utiles avec 6*128=768 mesures.\n",
    "\n",
    "C'est sur ces données brutes que nous allons appliquer des méthodes d'apprentissage.\n",
    "\n",
    "## Objectifs\n",
    "Cette deuxième étape s'intéresse aux données brutes. Est-il possible d'économiser le travail préliminaire de définition des variables métier en utilisant par exemple les ressources de décompositions systématiques sur une base d'ondelette ou un algorihtme d'apprentissage profond? L'enjeu est important, le calcul en temps réel des variables métier consomme beaucoup d'énergie et l'objectif serait de pouvoir embarquer (faire porter) un système économe défini par exemple par un réseau de neurones après son apprentissage.\n",
    "\n",
    "Comme pour les données métier, l'étude commence par une exploration: affichage et visualisation des données, réduction de dimension.\n",
    "\n",
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import time\n",
    "import utils.load as ul\n",
    "\n",
    "\n",
    "# Plot et Display\n",
    "import utils.illustration as uil\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "# Exploration\n",
    "import sklearn.decomposition as sdec \n",
    "import sklearn.preprocessing as sprep\n",
    "import sklearn.discriminant_analysis as sda\n",
    "\n",
    "#Prediction\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prise en charge des données\n",
    "## Source\n",
    "\n",
    "Les données sont celles originales du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle peuvent être téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Elles contiennent deux jeux de dimensions différentes, chacun partagé en apprentissage et test.\n",
    "\n",
    "1. Multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(N, 128, 9)$\n",
    "2. Unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(N, 1152)*\n",
    "        \n",
    "Deux objets différents sont construits pour définir la variable $Y$ réponse car les librairies `Scikit-learn` et `Keras` prennent en compte des structures différentes: \n",
    "    \n",
    "1. `Scikit-Learn`  Un vecteur de dimension $(N, 1)$ avec, pour chaque individu le numéro du label de l'activité de 0 à 5.\n",
    "2. `Keras` Une matrice de dimension $(N, 6)$ des indicatrices (0 ou 1) des modalités de $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structurer les données\n",
    "Définir le chemin d'accès aux données puis les fonctions utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "CMAP = plt.get_cmap(\"Set1\")\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "LABELS = [ACTIVITY_DIC[c] for c in range(1,7)]\n",
    "COLOR_DIC = {v:CMAP(k-2) if v!=\"WALKING\" else CMAP(10) for k,v in ACTIVITY_DIC.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = ul.load_signals(\"train\", SIGNALS), ul.load_signals(\"test\", SIGNALS)\n",
    "N_train, N_dim, N_signaux  = X_train.shape\n",
    "N_test, _, _  = X_test.shape\n",
    "\n",
    "\n",
    "#Flatten_data\n",
    "X_train_flatten = X_train.reshape((N_train, N_dim*N_signaux), order=\"F\")\n",
    "X_test_flatten = X_test.reshape((N_test, N_dim*N_signaux), order=\"F\")\n",
    "\n",
    "# Label Vector\n",
    "Y_train, Y_test = ul.load_y(\"train\"), ul.load_y(\"test\")\n",
    "N_per_activity_train = collections.Counter(Y_train)\n",
    "\n",
    "\n",
    "\n",
    "Y_train_label = np.array([ACTIVITY_DIC[y] for y in Y_train])\n",
    "Y_test_label = np.array([ACTIVITY_DIC[y] for y in Y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse : \" + str(Y_train_label.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_sample_per_activity = dict([(v,50) for k,v in ACTIVITY_DIC.items()])\n",
    "linestyle_per_activity = dict([(v,\"solid\") for k,v in ACTIVITY_DIC.items()])\n",
    "linewidth_per_activity = dict([(v,1)for k,v in ACTIVITY_DIC.items()])\n",
    "\n",
    "fig = plt.figure(figsize=(18,18))    \n",
    "uil.plot_signaux(fig, X_train, Y_train_label, SIGNALS, COLOR_DIC, nb_sample_per_activity, \n",
    "             linestyle_per_activity, linewidth_per_activity, figdim1 = 3, figdim2 = 3, shuffle=True, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Par signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "nb_sample = 10\n",
    "isignal=0    \n",
    "uil.plot_signal_per_activity(fig, X_train, Y_train_label, nb_sample, isignal, SIGNALS, COLOR_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Observer l'un des signaux, par exemple `Walking upstairs`. Qu'est ce qui fait que, pour ces signaux ou courbes, une métrique euclidienne classique ($L_2$) est inopérante? \n",
    "\n",
    "**Q** Corrélativement pourquoi est-il intéressant de décomposer un signal dans le domaine des fréquances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse en composantes principales\n",
    "Les ACP ne sont pas réduites. Refaire les calculs après réduction des variables : signaux.\n",
    "\n",
    "## Sur un Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = sdec.PCA()\n",
    "isignal = 4\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"ACP Sur signal : \" +signal)\n",
    "X_r = pca.fit_transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, pca, X_r, whis=100)\n",
    "fig.suptitle(\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: les diagrammes boîtes sont très perturbés par les distributions des composantes avec une très forte concentration autour de 0 et énormément de valeurs atypiques. D'où l'utilisation du paramètre `whis=100` pour rallonger les moustaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors=[COLOR_DIC[y] for y in Y_train_label]\n",
    "markersizes = [20 for _ in range(N_train)]\n",
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_pca(ax,X_r, pca, 1, 2, colors, markersizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sur tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = sdec.PCA()\n",
    "print(\"ACP Sur tous les signaux\")\n",
    "X_r = pca.fit_transform(X_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, pca, X_r)\n",
    "fig.suptitle(\"Résultat ACP sur tous les signaux\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_pca(ax,X_r, pca, 1, 2, colors, markersizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Analyse factorielle discriminante\n",
    "\n",
    "## Sur un signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isignal = 0\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"LDA Sur signal : \" +signal)\n",
    "\n",
    "method = sda.LinearDiscriminantAnalysis() \n",
    "lda=method.fit(X_train[:,:,isignal],Y_train_label)\n",
    "X_r2=lda.transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2, count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    uil.plot_pca(ax,X_r2, lda, nbc, nbc2, colors, markersizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sur tous les signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LDA Sur tous les signaux\")\n",
    "\n",
    "method = sda.LinearDiscriminantAnalysis() \n",
    "lda=method.fit(X_train_flatten,Y_train_label)\n",
    "X_r2=lda.transform(X_train_flatten)\n",
    "\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2, count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    uil.plot_pca(ax,X_r2, lda, nbc, nbc2, colors, markersizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction sur les signaux\n",
    "\n",
    "On effectue des prédictions directement sur la concatenation de tous les signaux avec les méthodes de SVM (lineaire et noyaux gaussien) ainsi que Random Forest et Gradient Boosting.\n",
    "\n",
    "Contrairement aux données métiers on observe que l'on obtient des meilleurs résultats avec des méthodes non-linéaires.\n",
    "\n",
    "## Svm Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = LinearSVC()\n",
    "method.fit(X_train_flatten,Y_train)\n",
    "score = method.score(X_test_flatten, Y_test)\n",
    "ypred = method.predict(X_test_flatten)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Noyau Gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = SVC()\n",
    "method.fit(X_train_flatten, Y_train)\n",
    "score = method.score(X_test_flatten, Y_test)\n",
    "ypred = method.predict(X_test_flatten)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = RandomForestClassifier(n_estimators = 300, n_jobs=-1)\n",
    "method.fit(X_train_flatten, Y_train)\n",
    "score = method.score(X_test_flatten, Y_test)\n",
    "ypred = method.predict(X_test_flatten)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = xgb.XGBClassifier()\n",
    "method.fit(X_train_flatten, Y_train)\n",
    "score = method.score(X_test_flatten, Y_test)\n",
    "ypred = method.predict(X_test_flatten)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Comparer ces performances avec celles obtenues dans la première partie sur les données métier.\n",
    "\n",
    "\n",
    "L'objectif de la section suivante est de voir si, à partir d'une décomposition des signaux sur une base d'ondelettes, on peut améliorer la qualité de prédiction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Décomposition en ondelettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "from pywt import wavedec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Illustration des coefficients\n",
    "\n",
    "### Coefficients pour un signal par comportement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train==act)[0][:sample_to_plot]) for act in range(1,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for ip, (act , index) in enumerate(index_per_act_dict.items()):\n",
    "    ax=fig.add_subplot(2,3,ip+1)\n",
    "    coef = pywt.wavedec(X_train_flatten[index,:1024], 'db1')\n",
    "    uil.coef_pyramid_plot(ax, coef[1:]) ;\n",
    "\n",
    "    ax.set_title(ACTIVITY_DIC[act]);\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelle base d'ondelettes est utilisée ?  Remarquez vous des différences de comportement des coefficients d ondelettes selon les \n",
    "classes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot des coefficients par niveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_db_list = pywt.wavedec(X_train_flatten, 'db1')\n",
    "uil.plot_boxplot_coef_concat_per_signal(X_train_db_list, Y_train, labels=LABELS, activity_dic=ACTIVITY_DIC, \n",
    "                             color_dic=COLOR_DIC, N_per_activity_train= N_per_activity_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Ces boxplots sont-ils cohérents avec les observations précédentes sur un signal par classe ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACP sur les coefficients d'ondelettes\n",
    "\n",
    "### Avec tous les coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_db = np.concatenate(pywt.wavedec(X_train_flatten, 'db1'), axis=1)\n",
    "X_test_db = np.concatenate(pywt.wavedec(X_test_flatten, 'db1'), axis=1)\n",
    "\n",
    "## ACP \n",
    "pca = sdec.PCA()\n",
    "X_train_db_pca = pca.fit_transform(X_train_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, pca, X_train_db_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2, count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    uil.plot_pca(ax,X_train_db_pca, pca, nbc, nbc2, colors, markersizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Comparer ces résultats avec l'ACP sur tous les signaux réalisée précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec les coefficients seuillés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cA10, cD10, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = pywt.wavedec(X_train_flatten, 'db1')\n",
    "\n",
    "sigma=0.01\n",
    "thresh = sigma*np.sqrt(2*np.log((X_train_flatten.shape[1])))\n",
    "\n",
    "# On seuille seulement les coefficients de détail : \n",
    "cD10=pywt.threshold(cD10, thresh, 'soft')\n",
    "cD9=pywt.threshold(cD9, thresh, 'soft')\n",
    "cD8=pywt.threshold(cD8, thresh, 'soft')\n",
    "cD7=pywt.threshold(cD7, thresh, 'soft')\n",
    "cD6=pywt.threshold(cD6, thresh, 'soft')\n",
    "cD5=pywt.threshold(cD5, thresh, 'soft')\n",
    "cD4=pywt.threshold(cD4, thresh, 'soft')\n",
    "cD3=pywt.threshold(cD3, thresh, 'soft')\n",
    "cD2=pywt.threshold(cD2, thresh, 'soft')\n",
    "cD1=pywt.threshold(cD1, thresh, 'soft')\n",
    "\n",
    "X_train_dbth = np.concatenate((cA10, cD10, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cA10, cD10, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = pywt.wavedec(X_test_flatten, 'db1')\n",
    "\n",
    "sigma=0.01\n",
    "thresh = sigma*np.sqrt(2*np.log((X_test_flatten.shape[1])))\n",
    "\n",
    "# On seuille seulement les coefficients de détail : \n",
    "cD10=pywt.threshold(cD10, thresh, 'soft')\n",
    "cD9=pywt.threshold(cD9, thresh, 'soft')\n",
    "cD8=pywt.threshold(cD8, thresh, 'soft')\n",
    "cD7=pywt.threshold(cD7, thresh, 'soft')\n",
    "cD6=pywt.threshold(cD6, thresh, 'soft')\n",
    "cD5=pywt.threshold(cD5, thresh, 'soft')\n",
    "cD4=pywt.threshold(cD4, thresh, 'soft')\n",
    "cD3=pywt.threshold(cD3, thresh, 'soft')\n",
    "cD2=pywt.threshold(cD2, thresh, 'soft')\n",
    "cD1=pywt.threshold(cD1, thresh, 'soft')\n",
    "\n",
    "X_test_dbth = np.concatenate((cA10, cD10, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ACP \n",
    "pca = sdec.PCA()\n",
    "X_train_dbth_pca = pca.fit_transform(X_train_dbth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, pca, X_train_db_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2, count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    uil.plot_pca(ax,X_train_dbth_pca, pca, nbc, nbc2, colors, markersizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Comparez avec les résultats précédents ? Les différences sont-elles significatives ? Essayez de modifier le seuil, puis de ne pas seuiller les premiers niveaux de détail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction des classes avec les coefficients d'ondelettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Linéaire\n",
    "\n",
    "En considérant tous les coefficients, le score est identique (56.73% contre 56.93%) aux données brutes.\n",
    "On obtient un net gain en utilisant les coefficients seuillés : 66.34%\n",
    "\n",
    "Cependant ces scores sont toujours loins des 96% des données métiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = LinearSVC()\n",
    "method.fit(X_train_db, Y_train)\n",
    "score = method.score(X_test_db, Y_test)\n",
    "ypred = method.predict(X_test_db)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = LinearSVC()\n",
    "method.fit(X_train_dbth, Y_train)\n",
    "score = method.score(X_test_dbth, Y_test)\n",
    "ypred = method.predict(X_test_dbth)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Noyau Gaussien\n",
    "\n",
    "En utilisant un noyau non lineaire, les résultats sont globalement meilleurs. (> 70%)\n",
    "Cependant l'utilisation de cette méthode sur les coefficients d'ondelettes (seuillés ou non) produit de moins bons scores (resp 72.07 et 74.34) que sur les signaux bruts (76.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = SVC()\n",
    "method.fit(X_train_db, Y_train)\n",
    "score = method.score(X_test_db, Y_test)\n",
    "ypred = method.predict(X_test_db)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = SVC()\n",
    "method.fit(X_train_dbth, Y_train)\n",
    "score = method.score(X_test_dbth, Y_test)\n",
    "ypred = method.predict(X_test_dbth)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = RandomForestClassifier()\n",
    "method.fit(X_train_db, Y_train)\n",
    "score = method.score(X_test_db, Y_test)\n",
    "ypred = method.predict(X_test_db)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = RandomForestClassifier()\n",
    "method.fit(X_train_dbth, Y_train)\n",
    "score = method.score(X_test_dbth, Y_test)\n",
    "ypred = method.predict(X_test_db)\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(Y_test, ypred), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = xgb.XGBClassifier()\n",
    "method.fit(X_train_db, Y_train)\n",
    "score = method.score(X_test_db, Y_test)\n",
    "ypred = method.predict(X_test_db)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "method = xgb.XGBClassifier()\n",
    "method.fit(X_train_dbth, Y_train)\n",
    "score = method.score(X_test_dbth, Y_test)\n",
    "ypred = method.predict(X_test_dbth)\n",
    "ypred_label = np.array([ACTIVITY_DIC[y] for y in ypred])\n",
    "te = time.time()\n",
    "t_total = te-ts\n",
    "\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.crosstab(Y_test_label, ypred_label, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Comparez les performances de ces différentes méthodes avec celles obtenues sur les signaux bruts et celles obtenues sur les données métier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "L'analyse des données brutes (signaux)  ou de leur décomposition sur une base d'ondelettes ne permet pas de retrouver les performances obtenues sur les données métiers. Une autre approche consistera à utiliser l'apprentissage profond sur les  signaux afin d'essayer de retrouver des performances plus proches de celles obtenues avec les données métier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "957px",
    "left": "0px",
    "right": "1605.78px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
