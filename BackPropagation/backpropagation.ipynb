{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional & Deep Learning : Backpropagation in Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://github.com/m2dsupsdlclass/lectures-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Backpropagation? \n",
    "\n",
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this TP are : \n",
    "   * Understand the theopry of the backpropagation algorithm\n",
    "   * Implement logistic regression and multi perceptron layer algorithm using backpropagation algorithm with numpy\n",
    "   * Use Keras to apply the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "    ax0.grid(False)\n",
    "    ax0.axis('off')\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Dataset\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "N = reduce(lambda x,y: x*y,digits.images[0].shape)\n",
    "print(\"Image dimension : N=%d\"%N)\n",
    "K = len(set(digits.target))\n",
    "print(\"Number of classes : K=%d\"%K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "fig =plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "ax.set_title(\"image label: %d\" % digits.target[sample_index])\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data dimension and type\")\n",
    "print(\"X_train : \" + str(X_train.shape) + \", \" +str(X_train.dtype))\n",
    "print(\"y_train : \" + str(y_train.shape) + \", \" +str(y_train.dtype))\n",
    "print(\"X_test : \" + str(X_test.shape) + \", \" +str(X_test.dtype))\n",
    "print(\"y_test : \" + str(y_train.shape) + \", \" +str(y_train.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Utils Function\n",
    "\n",
    "Write utils function that will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding function\n",
    "\n",
    "$$\n",
    "OneHotEncoding(4,N_{class}=K) = \n",
    "\\begin{bmatrix}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  1\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Exercise :** Implement the **one hot encoding** function of an integer array for a fixed number of classes (similar to keras' `to_categorical`):  \n",
    "Ensure that your function works for several vectors at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the one_hot function\n",
    "def one_hot(y, n_classes):\n",
    "    ##\n",
    "    return ohy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/one_hot_encoding.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the solution works on 1D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohy = one_hot(y=3,n_classes=10)\n",
    "print(\"Expected : array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) \\n\")\n",
    "print(\"Computed  :\" + str(ohy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the solution works on 2D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohY = one_hot(n_classes=10, y=[0, 4, 9, 1])\n",
    "print(\"Expected : [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] \\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \\n  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\")\n",
    "print(\"Computed  :\" + str(ohY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Exercise :** Implement the softmax function.  \n",
    "Ensure that your function works for several vectors at a time.  \n",
    "Hint : use the *axis* and *keepdims* argument of the numpy function `np.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keepdims option\n",
    "x = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "\n",
    "print(\"Sum all elements of array :\")\n",
    "sx = np.sum(x)\n",
    "print(sx)\n",
    "\n",
    "print(\"Sum all elements over axis (dimension) :\" )\n",
    "sx = np.sum(x, axis=-1)\n",
    "print(str(sx), str(\", Dimension :\") ,str(sx.shape))\n",
    "\n",
    "print(\"Sum all elements over axis and with keepdims (dimension) :\" )\n",
    "sx = np.sum(x, axis=-1,  keepdims=True)\n",
    "print(str(sx), str(\", Dimension :\") ,str(sx.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the softmax function\n",
    "def softmax(x):\n",
    "    ###\n",
    "    return softmaxX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you function workds on 1D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 2, -3]\n",
    "sx = softmax(x)\n",
    "print(\"Expected : [9.99662391e-01 3.35349373e-04 2.25956630e-06]\")\n",
    "print(\"Computed \" + str(sx))\n",
    "print(\"Value Sum to one : %d\" %np.sum(sx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you function workds on 2D array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "sX = softmax(X)\n",
    "print(\"Expected : [[9.99662391e-01 3.35349373e-04 2.25956630e-06] \\n [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\")\n",
    "print(\"Value found\" + str(sX))\n",
    "print(\"Value Sum to one : \" + str(np.sum(sX, axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood function\n",
    "\n",
    "The definition of the log likelihood function according to course's notation : \n",
    "$$ -\\log (f(x))_y = -  \\sum_{k=1}^K  \\mathbb{1}_{y=k} \\log (f(x))_k= \\ell( f(x),y)\n",
    "$$\n",
    "\n",
    "where $(f(x))_k =  \\mathbb{P}(Y=k/x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**:  \n",
    "Write a function that compute the mean negative likelihood of a group of predictions `Y_true` and `Fx`, where `Y_true`and `Fx` are the one-hot encoded representation of the label and the predictions. i.e. :\n",
    "\n",
    "* `Y_true` is the one-hot encoded representation of $y$\n",
    "* `Fx` is the output of a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the negative_log_likelihood function\n",
    "EPSILON = 1e-8\n",
    "def NegLogLike(Y_true, Fx):\n",
    "    ###\n",
    "    return nll_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/negative_log_likelihood_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the negative log likelihood for a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple case\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [.99, 0.01, 0] \n",
    "nll1 = NegLogLike(ohy_true, fx)\n",
    "print(\"Good prediction :\")\n",
    "print(\"Exepected value : 0.01005032575249135\")\n",
    "print(\"COmputed : \" + str(nll1) )\n",
    "\n",
    "# Case with bad prediction\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [0.01, .99, 0] \n",
    "nll2 = NegLogLike(ohy_true, fx)\n",
    "print(\"Bad prediction (Higher loss function):\")\n",
    "print(\"Exepected value : 4.605169185988592\")\n",
    "print(\"COmputed : \" + str(nll2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can handle zero-case prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero case\n",
    "ohy_true = [1, 0, 0]\n",
    "fx = [0, 0.01, 0.99] \n",
    "nll3 = NegLogLike(ohy_true, fx)\n",
    "print(\"Good prediction :\")\n",
    "print(\"COmputed : \" + str(nll3) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood for several predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohY_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Fx = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "nll4 = NegLogLike(ohY_true, Fx)\n",
    "print(\"Bad prediction (Higher loss function):\")\n",
    "print(\"Exepected value : 0.0033501019174971905\")\n",
    "print(\"COmputed : \" + str(nll4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    ###\n",
    "    return sigX\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    ###\n",
    "    return dsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the sigmoid function and tis derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "ax.plot(x, sigmoid(x), label='sigmoid')\n",
    "ax.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD **one sample at a time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Complete the `LogisticRegression` class by following these steps (Use the function you write above) :\n",
    "\n",
    "**Notation** : $x \\in \\mathbb{R}^N$, $y \\in [0,...,K]$, $W \\in \\mathbb{R}^{K,N}$, $b \\in \\mathbb{R}^K$\n",
    "\n",
    "\n",
    "1.  Implement the `forward` function:  \n",
    "$$f(x) = softmax(\\mathbf{W} x + b)$$\n",
    "\n",
    "2. Implement the `grad_loss`function which computes the derivative of the loss function (for an for an $x$ and its corresponding expected output $y$) with respect to the parameters of the function$W$ and $b$ :\n",
    "\n",
    "\\begin{array}{ll} \n",
    "   grad_W &= \\frac{d}{dW} [-\\log (f(x))_y] \\\\\n",
    "   grad_b &= \\frac{d}{db} [-\\log (f(x))_y]\n",
    "\\end{array}\n",
    "\n",
    "**Hint**  \n",
    "\\begin{array}{ll}\n",
    "    \\frac{d}{dW_{i,j}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      [f(x)_{y}-1]*x_j, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}*x_j, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{db_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      f(x)_{y}-1, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "\\end{array}\n",
    "       \n",
    "\n",
    "3. Implement the `train` function which uses the grad function output to update $\\mathbf{W}$ and $b$ with traditional SGD update without momentum :\n",
    "\\begin{array}{ll}\n",
    "W &= W - \\lambda \\frac{d}{dW} [-\\log (f(x))_y]\\\\\n",
    "b &= b - \\lambda \\frac{d}{db} [-\\log (f(x))_y]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ###\n",
    "        return sZ\n",
    "    \n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        ###\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        ### \n",
    "    \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        nll = NegLogLike(one_hot(self.output_size, y), self.forward(x))\n",
    "        return nll\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        acc = np.mean(y_preds == y)\n",
    "        return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lr_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "lr = LogisticRegression(N, K)\n",
    "\n",
    "print(\"Evaluation of the unÂ£trained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Multi Layer Perceptron\n",
    "\n",
    "\n",
    "In this section we will implement a neural network model with one hidden layer using the sigmoid activation.\n",
    "You will implement the backpropagation algorithm (SGD with the chain rule) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Complete the `NeuralNet` class following these step\n",
    "\n",
    "**Notation** : $x \\in \\mathbb{R}^N$, $h \\in \\mathbb{R}^H$, $y \\in [0,...,K]$, $W^{h} \\in \\mathbb{R}^{H,N}$, $b^h \\in \\mathbb{R}^H$, $W^{o} \\in \\mathbb{R}^{K,H}$, $b^o \\in \\mathbb{R}^K$\n",
    "\n",
    "\n",
    "1. Implement `forward` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "\\begin{array}{lll} \n",
    "  \\mathbf{h} &= sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h}) &= sigmoid(z^h(x)) \\\\\n",
    "  f(x) &= softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o}) &= softmax(z^o(x))\\\\\n",
    "\\end{array}\n",
    "\n",
    "  which return $y$ if *keep_activation* = False and $y$, $h$ and  $z^h(x)$ otherwise.\n",
    "\n",
    "2. 2. Implement the `grad_loss` function which computes the derivative of the loss function (for an for an $x$ and its corresponding expected output $y$) with respect to the parameters of the function $W^h$, $b^h$, $W^o$ and $b^o$ :\n",
    "\n",
    "\\begin{array}{ll} \n",
    "   \\nabla_{W^{o}}loss &= \\frac{d}{dW^{o}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{b^{o}}loss &= \\frac{d}{db^{o}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{W^{h}}loss &= \\frac{d}{dW^{h}} [-\\log (f(x))_y] \\\\\n",
    "   \\nabla_{b^{h}}loss &= \\frac{d}{db^{h}} [-\\log (f(x))_y]\n",
    "\\end{array}\n",
    "\n",
    "**Hint**  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "    \\frac{d}{dz^0_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      [f(x)_{y}-1], & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{dW^o_{i,j}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      [f(x)_{y}-1]*h_j, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}*h_j, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{db^o_{i}} [-\\log (f(x))_y] &= \n",
    "    \\begin{cases}\n",
    "      f(x)_{y}-1, & \\text{if}\\ i=y \\\\\n",
    "      f(x)_{i}, & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{d}{dh_{j}} [-\\log (f(x))_y] &= \\nabla_{z^{o}}loss ~\\cdot~ W^o_{-,j} \\\\\n",
    "    \\frac{d}{dz^h_{j}} [-\\log (f(x))_y] &=  \\nabla_{z^{o}}loss ~\\cdot~ W^o_{.,j}  * dsigmoid(z^h_{j}) \\\\\n",
    "    \\frac{d}{dW^h_{j,l}} [-\\log (f(x))_y] &= \\nabla_{z^h}loss_j  *  x_l \\\\ \n",
    "    \\frac{d}{db^h_{j}} [-\\log (f(x))_y] &= \\nabla_{z^h}loss_j  \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    \n",
    "    def forward(self, X, keep_activation=False):\n",
    "        ###\n",
    "        rep = [fx, h, z_h] if keep_activation else fx\n",
    "        return rep\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        fx = self.forward(X)\n",
    "        ohy = one_hot(self.output_size, y)\n",
    "        nll = NegLogLike(ohy, fx)\n",
    "        return nll \n",
    "\n",
    "    def grad_loss(self, X, y_true):\n",
    "        ####\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        fx = self.forward(X)\n",
    "        if len(X.shape) == 1:\n",
    "            \n",
    "            yp = np.argmax(fx)\n",
    "        else:\n",
    "            yp = np.argmax(fx, axis=1)\n",
    "        return yp\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/nn_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10\n",
    "model = NeuralNet(N, H, K)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = model.loss(X_train, y_train)\n",
    "train_acc = model.accuracy(X_train, y_train)\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model for several epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, losses_test, accuracies, accuracies_test = [], [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "losses_test.append(model.loss(X_test, y_test))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    losses_test.append(model.loss(X_test, y_test))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss evolution per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(losses,label='train')\n",
    "ax.plot(losses_test,label='test')\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend(loc='best');\n",
    "ax.set_title(\"Training loss\");\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(accuracies, label='train')\n",
    "ax.plot(accuracies_test, label='test')\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.legend(loc='best');\n",
    "ax.set_title(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Tensorflow/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the same multi layer perceptron using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "n_features = 64\n",
    "n_classes = 10\n",
    "n_hidden = 10\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mlp_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a function that produce the same results that the plot_prediction function but with keras model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/plot_prediction_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare loss and accuracy of keras and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/compare_loss_acc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
