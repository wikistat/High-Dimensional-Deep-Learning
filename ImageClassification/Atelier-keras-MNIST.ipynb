{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional & Deep Learning : Image classification  on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "In this TP you will learn to : \n",
    "* Write multilayer perceptron and convolutional network with `Keras`and `Tensorflow`\n",
    "* Understand how `convolutional`, `max pooling`, `stride` and `padding`layers work.\n",
    "* Use these model to use classification problem on image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow.keras.utils as ku\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.preprocessing.image as k\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code lines allow you to check if your computer is using CPU or GPU ressources. <br>\n",
    "**Warning** : You won't be able to use GPU if another notebook is open and using GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used in this TP are the [MNIST DataBase](http://yann.lecun.com/exdb/mnist/).<br>\n",
    "They are composed of 70.000 images (60.000 for learning, 10.000 for test) of 28x28 pixels of handwritten digits from 0 to 9.<br>\n",
    "\n",
    "These data are directly available on the `Keras` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "N_train, N_x_pixels, N_y_pixels = X_train.shape\n",
    "N_test = X_test.shape[0]\n",
    "N_classes = len(set(Y_train))\n",
    "\n",
    "print(\"Train data : %d images  (%d/%d pixels)\" %(N_train, N_x_pixels, N_y_pixels))\n",
    "print(\"Test data : %d images  (%d/%d pixels)\" %(N_test, N_x_pixels, N_y_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "fig =plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(X_train[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "ax.set_title(\"image label: %d\" % Y_train[sample_index])\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification with Multi Layer Perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to learn an image classifier with a MLP model with the following architecture.\n",
    "\n",
    "* A Dense layer with 128 neurons and *relu* activation function\n",
    "* A Dropout Layer with 20% drop rate\n",
    "* A Dense layer with 128 neurons and *relu* activation function\n",
    "* A Dropout Layer with 20% drop rate\n",
    "* A Dense layer with 10 neurons (Number of classes ) and *softmax* activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "Some modification are required on the data to used them with our model. \n",
    "\n",
    "The first layer is a Dense Layer, which handle 1D vectors as an input. We must first reshape the 2D 28x28 images as a 1D $28*28=784$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape((N_train, N_x_pixels*N_y_pixels))\n",
    "X_test_flatten = X_test.reshape((N_test, N_x_pixels*N_y_pixels))\n",
    "N_dim_flatten = X_train_flatten.shape[1]\n",
    "print(\"Dimensions of flatten train images : %d X %d\" %(X_train_flatten.shape))\n",
    "print(\"Dimensions of flatten test images : %d X %d\" %(X_test_flatten.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du réseau\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, activation='relu', input_shape=(N_dim_flatten,)))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(128, activation='relu'))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(N_classes, activation='softmax'))\n",
    "# Réumé\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** The summary methods display the number of Param/weigths of the model. Retrieve these values with the formula seen in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now instantiate yout model by defining :\n",
    "* A optimizer : `RMSprop`\n",
    "* a loss function : `Categorical crossentropy`\n",
    "* Metric : This argument is an options, it allows to compute the metric you want to check the evolution of the training. Here we choose to compute the accuracy during the training\n",
    "\n",
    "**Note** : In Keras you can choose either \"sparse_categorical_crossentropy\", \"categorical_crossentropy\" loss or . The former handel 1D (NX1) vectors where each entry contains the label of the data, i.e [0,3,5,9,3,4,...]. The latter handle only one-hot encoding of this vector, ie a 2D vectors (NXN_classes) matrices. Keras has a `to_categorical`functions which allows to convert a vector to his one-hot encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=ko.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs=10\n",
    "ts = time.time()\n",
    "history = model.fit(X_train_flatten, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_flatten, Y_test))\n",
    "te = time.time()\n",
    "t_train_mpl = te-ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mpl = model.evaluate(X_test_flatten, Y_test, verbose=0)\n",
    "predict_mpl = model.predict(X_test_flatten)\n",
    "print('Test loss:', score_mpl[0])\n",
    "print('Test accuracy:', score_mpl[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_mpl )\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_mpl.argmax(1))), annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about these results?\n",
    "\n",
    "**Exercise** Normalize the data in order that they have values between 0 and 1 and run again the learning. What can you say about these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "In this part we will use convolution layers to build a convolutional classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data\n",
    "\n",
    "The convolution architecure expect images and not 1D vectors. However, some data formating are again required.\n",
    "\n",
    "A third dimension is required : the `channels` dimension which will allow to describe each pixel. In pour case this dimension's size is only 1 because the images are only defined with grey scale. However for colours images, each pixel is coded with several values (Images are generally encoded with 3 values (RGB channels)). \n",
    "\n",
    "Hence, we need to reshape the images from a 28x28 dimension to a 28X28X1 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv = np.expand_dims(X_train,axis=-1)\n",
    "X_test_conv = np.expand_dims(X_test,axis=-1)\n",
    "X_train_conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Edge detection\n",
    "\n",
    "We will first check the transformation apply by a convolution layer.\n",
    "\n",
    "In the following coded we define a convolutional network define of only one filter for which we manually defined the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_filter = np.array([\n",
    "        [0.2, -0.2, 0],\n",
    "        [0.2, -0.2, 0],\n",
    "        [0.2, -0.2, 0],\n",
    "    ])\n",
    "\n",
    "def my_init_filter(shape, conv_filter = conv_filter, dtype=None, partition_info=None):\n",
    "    xf,yf = conv_filter.shape\n",
    "    array = conv_filter.reshape(xf, yf, 1, 1)\n",
    "    return array\n",
    "my_init_filter(0).shape\n",
    "\n",
    "conv_edge = km.Sequential([\n",
    "    kl.Conv2D(kernel_size=(3,3), filters=1, kernel_initializer=my_init_filter, input_shape=(28, 28, 1))   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Q** Note that in  `my_init_filter` two dimensions have been added to the conv filter. What these dimensions represent?\n",
    " \n",
    " The following code allows to display the image, the filtern and the convoluted image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_in = np.expand_dims(x, 0)\n",
    "img_out = conv_edge.predict(img_in)\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax0.imshow(img_in[0,:,:,0], cmap=\"binary\")\n",
    "ax0.set_title(\"Image originale\")\n",
    "ax0.grid(False)\n",
    "\n",
    "norm_conv_filter = (conv_filter-conv_filter.min())/conv_filter.max()\n",
    "ax1.imshow(norm_conv_filter.astype(np.uint8), cmap=\"binary\")\n",
    "ax1.set_title(\"Filtre\")\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.imshow(img_out[0,:,:,0].astype(np.uint8), cmap=\"binary\")\n",
    "ax2.set_title(\"Image Filtre\")\n",
    "ax2.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What do you see? Are the output image coherent according to the designed filter\n",
    "\n",
    "**Exercise** Change the code in order to test different filter (to detect horizontal edge, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides and Padding\n",
    "\n",
    "We will now study the effect on `strides`et `padding` arguments on the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filter = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ])\n",
    "\n",
    "def my_init_filter(shape, conv_filter = conv_filter, dtype=None, partition_info = None):\n",
    "    xf,yf = conv_filter.shape\n",
    "    array = conv_filter.reshape(xf, yf, 1, 1)\n",
    "    return array\n",
    "my_init_filter(0).shape\n",
    "\n",
    "conv_sp = km.Sequential([\n",
    "    kl.Conv2D(kernel_size=(3,3), filters=1, kernel_initializer=my_init_filter, input_shape=(28, 28, 1),\n",
    "           strides=2, padding=\"SAME\") ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What is the effect of the convolutional filter definad here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_in = np.expand_dims(x, 0)\n",
    "img_out = conv_sp.predict(img_in)\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax0.imshow(img_in[0,:,:,0].astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax0.grid(False)\n",
    "\n",
    "norm_conv_filter = (conv_filter-conv_filter.min())/conv_filter.max()\n",
    "ax1.imshow(norm_conv_filter.astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.imshow(img_out[0,:,:,0].astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax2.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Check the dimension of the output images. Are they coherent? <br>\n",
    "**Exercise** Change both *strides* and *padding* arguments and understant the effect of this change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Write a similar coded than above to check and understand the behaviour of the `max pooling` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/max_pooling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** What are the dimension of the output image? Is this normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Convolutional Network (ConvNet)*\n",
    "\n",
    "We will now build convolutional network and see the performance on this kind of model on  iamge classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5\n",
    "\n",
    "On teste dans un premier temps le modèle LeNet5 proposer par LeCun et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet5model = km.Sequential()\n",
    "LeNet5model.add(kl.Conv2D(filters = 6, kernel_size = 5, strides = 1, activation = 'tanh',\n",
    "input_shape = (28,28,1)))\n",
    "LeNet5model.add(kl.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "LeNet5model.add(kl.Conv2D(filters = 16, kernel_size = 5,strides = 1, activation = 'tanh'))\n",
    "LeNet5model.add(kl.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "LeNet5model.add(kl.Flatten())\n",
    "LeNet5model.add(kl.Dense(units = 120, activation = 'tanh'))\n",
    "LeNet5model.add(kl.Dense(units = 84, activation = 'tanh'))\n",
    "LeNet5model.add(kl.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "LeNet5model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Retrieve 'manually' the number of parameters of this model.\n",
    "\n",
    "**Question** What can you say abbout the total number of parameters compare with the MLP model defined before? Wich layer have the most parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "batch_size=128\n",
    "epochs=30\n",
    "LeNet5model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=ko.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "ts=time.time()\n",
    "LeNet5model.fit(X_train_conv, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_conv, Y_test))\n",
    "te=time.time()\n",
    "t_train_conv = te-ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Why is the training time longer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conv = LeNet5model.evaluate(X_test_conv, Y_test, verbose=0)\n",
    "predict_conv = LeNet5model.predict(X_test_conv)\n",
    "print('Test loss:', score_conv[0])\n",
    "print('Test accuracy:', score_conv[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_conv )\n",
    "\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_conv.argmax(1))), annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex architecture\n",
    "\n",
    "#### Network\n",
    "\n",
    "We will now design a more complex architecture to try to improve the results of the classification :\n",
    "\n",
    "* A Conv2D layer with 32-3X3 filters and a `Relu` activation function.\n",
    "* A Conv2D layer with642-3X3 filters and a `Relu` activation function.\n",
    "* A MaxPooling layer with 2X2 window.\n",
    "* A Dropout layer with a 25% drop rate.\n",
    "* A flatten layer.\n",
    "* A Dense layer with 128 neurons  and a `Relu` activation function.\n",
    "* A Dropout layer with a 50% drop rate.\n",
    "* A Dense layer with 10 neurons  and a `softmax` activation function.\n",
    "\n",
    "\n",
    "**Exercise** write these models and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mnist_conv_architecture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conv = model.evaluate(X_test_conv, Y_test_cat, verbose=0)\n",
    "predict_conv = model.predict(X_test_conv)\n",
    "print('Test loss:', score_conv[0])\n",
    "print('Test accuracy:', score_conv[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_conv )\n",
    "\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_conv.argmax(1))), annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comments the results.\n",
    "\n",
    "**Q** How to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
